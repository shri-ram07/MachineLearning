{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ0_ZUnO7l9W"
      },
      "source": [
        "# ***PyTorch***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8I7H189ulFV_"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "scnDF6BblPJO",
        "outputId": "c206180e-9f61-4ca0-c8cd-d73bf60aec1f"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHc84gNalVsE",
        "outputId": "ac7e9f43-437a-4511-f117-2f1408f50017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.get_device_name())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1abfo4_RlfxV"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV3pA1COlj30"
      },
      "outputs": [],
      "source": [
        "#TENSOR AND SOME OPERATIONS\n",
        "x = torch.rand(3,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe8yFGfRl1Qq",
        "outputId": "3d4965bc-1229-449b-c91a-9e13afbc5798"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 3])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVxsJSarl3RM",
        "outputId": "c1ef0224-28ab-4418-c8b1-8e8c6a5ea199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.3035, 0.7667, 0.5050],\n",
              "        [0.0415, 0.9045, 0.0778],\n",
              "        [0.0913, 0.8962, 0.0334]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vupGgsPl9wL",
        "outputId": "32a480f6-0d8e-4782-975f-6b67d12af59e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ones function will generate only one as a element \n",
            " \n",
            " tensor([[1., 1.],\n",
            "        [1., 1.]]) \n",
            " \n",
            " Same for zeors \n",
            " \n",
            " tensor([[0., 0.],\n",
            "        [0., 0.]])  \n",
            " \n",
            " .rand will generate random numbers with a given dimenssion \n",
            " \n",
            " tensor([[0.6742, 0.3762],\n",
            "        [0.4606, 0.0335]]) \n",
            " \n",
            " , .arange will generate in a ceratin range with given steps \n",
            " \n",
            " tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]) \n",
            " \n",
            " and , .tensor can be used to create a tensor by and list or arrays \n",
            " \n",
            " tensor([1, 2, 3, 4, 5, 6, 7])\n"
          ]
        }
      ],
      "source": [
        "print(f\"Ones function will generate only one as a element \\n \\n {torch.ones(2,2)} \\n \\n Same for zeors \\n \\n {torch.zeros(2,2)}  \\n \\n .rand will generate random numbers with a given dimenssion \\n \\n {torch.rand(2,2)} \\n \\n , .arange will generate in a ceratin range with given steps \\n \\n {torch.arange(1,10 )} \\n \\n and , .tensor can be used to create a tensor by and list or arrays \\n \\n {torch.tensor([1,2,3,4,5,6,7])}\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4RUT0VQl6Kx"
      },
      "outputs": [],
      "source": [
        "y = np.random.rand(3,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDfUoYY-pNAD",
        "outputId": "de0dd5de-ef6a-49e4-bfbc-4f6191e430c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.80143698, 0.07280508, 0.73898993],\n",
              "       [0.12306541, 0.06445327, 0.63987317],\n",
              "       [0.40235994, 0.48981652, 0.70747925]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RXPt5aXpP0e",
        "outputId": "3b39bba9-95fd-4b15-c562-22a1de127b47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.8014, 0.0728, 0.7390],\n",
            "        [0.1231, 0.0645, 0.6399],\n",
            "        [0.4024, 0.4898, 0.7075]], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "#convert y to tensor\n",
        "x = torch.from_numpy(y)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmmivSmrDTy4"
      },
      "outputs": [],
      "source": [
        "a = np.arange(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdAHruSEDXnF",
        "outputId": "18d2c76f-1aca-4d64-c9d1-27d365233ffe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_wOtZByDYt0"
      },
      "outputs": [],
      "source": [
        "b = torch.rand(1,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d1LhBcnDjDt",
        "outputId": "78b3eb1d-9f70-4844-9267-5eceac2d5518"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.1354, 0.6506, 0.3886, 0.5585, 0.9173, 0.5654, 0.6666, 0.1433, 0.1222,\n",
              "         0.9678]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4E6sPurDlId",
        "outputId": "17513633-ad0a-48c7-90ed-5d7ff5a57ae7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.1354, 0.6506, 0.3886, 0.5585, 0.9173, 0.5654, 0.6666, 0.1433, 0.1222,\n",
            "        0.9678])\n"
          ]
        }
      ],
      "source": [
        "#tensor slicing\n",
        "print(b[0,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cTT2jyZD9B3",
        "outputId": "646cb6ad-e0a0-4527-b011-9c94ba0f1b67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6665869951248169"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b[0,6].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4JzxTFoEGzi"
      },
      "outputs": [],
      "source": [
        "#resizing a torch tensor\n",
        "\n",
        "b = b.view(-1,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-O7sCNeEQck",
        "outputId": "a39641c9-62c5-42f8-e29c-dac78ba503db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.1354, 0.6506],\n",
              "        [0.3886, 0.5585],\n",
              "        [0.9173, 0.5654],\n",
              "        [0.6666, 0.1433],\n",
              "        [0.1222, 0.9678]])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#-1 for automatic selection\n",
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFaCeseGEV37",
        "outputId": "765fe0fb-a67d-4f6e-d981-bb3f30083135"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "#Conversion of tensor to np array and vice versa\n",
        "a = np.arange(0,16).reshape(4,4)\n",
        "b = torch.arange(0,10).view(2,5)\n",
        "\n",
        "a = torch.from_numpy(a)  #now it is tensor\n",
        "b = b.numpy()  #Now it is Numpy array\n",
        "\n",
        "print(type(a))\n",
        "\n",
        "print(type(b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "BmuaCF0cFuiN",
        "outputId": "af9870e0-2c51-4f29-846c-87f908c124de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "cpu\n",
            "cuda:0\n",
            "cpu\n",
            "[0. 0. 0.]\n",
            "tensor([1., 1., 1.], device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"cuda:0\\ncpu\\n[0. 0. 0.]\\ntensor([1., 1., 1.], device='cuda:0', dtype=torch.float64)\""
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#if numpy array and tensor are in same cpu or gpu then all the data will be saved in same location if we convert it to another\n",
        "a = np.zeros(3)\n",
        "b = torch.from_numpy(a)\n",
        "print(a.device)\n",
        "print(b.device)\n",
        "\n",
        "\"\"\"b.add_(2)\n",
        "print(a)\n",
        "print(b)\n",
        "cpu\n",
        "cpu\n",
        "[2. 2. 2.]\n",
        "tensor([2., 2., 2.], dtype=torch.float64)\"\"\"\n",
        "#here we have only added 2 in tensor but as you can see in the output , it has also updated the values of numpy array also\n",
        "#So for this we have to store in different different locations\n",
        "\n",
        "b = torch.from_numpy(a).to(\"cuda\")    #we can also use device attribute while declaring a tensor and .to is used to move from one device to another\n",
        "print(b.device)\n",
        "print(a.device)\n",
        "\n",
        "b.add_(1)\n",
        "print(a)\n",
        "print(b)\n",
        "\n",
        "\"\"\"cuda:0\n",
        "cpu\n",
        "[0. 0. 0.]\n",
        "tensor([1., 1., 1.], device='cuda:0', dtype=torch.float64)\"\"\"\n",
        "\n",
        "#and also we can not convert GPU tensors to numpy array so for that first we have to move it from gpu to cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53JdC_jVHROK"
      },
      "outputs": [],
      "source": [
        "#requires_grad parameter\n",
        "# used to help in finding the gradient of that tensor so we have to make it trure while intializing tensor\n",
        "x = torch.tensor([1.,2.,3.,4.] , requires_grad= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ztwXOjoInaA",
        "outputId": "3c758330-3e5c-4569-c2c5-514ca173618e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-0.7917,  0.9544, -1.3850, -0.9299,  1.0775,  0.2821,  0.2183,  0.2462,\n",
            "        -0.1998,  1.7824], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "#Auto Gradient in torch\n",
        "\n",
        "x = torch.randn(10 , requires_grad=True)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_zG5bZeI80N",
        "outputId": "f5124787-8612-4871-ef65-477a23a690af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1.2083, 2.9544, 0.6150, 1.0701, 3.0775, 2.2821, 2.2183, 2.2462, 1.8002,\n",
            "        3.7824], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "y = x+2\n",
        "print(y)\n",
        "#we will get a grad_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lCRbndGJswl",
        "outputId": "35ba0008-7b41-4917-e565-0ea53dc8987a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
            "        0.1000])\n"
          ]
        }
      ],
      "source": [
        "#we can calculate gradiend directly by using .backward function\n",
        "# When y is not a scalar, you need to provide a gradient argument to y.backward()\n",
        "# or convert y to a scalar before calling backward().\n",
        "# Calculating the mean of y is a common way to get a scalar value\n",
        "# so that you can compute the gradient of the mean with respect to the input tensor x.\n",
        "# y_scalar.backward() computes the gradient of y_scalar (the mean of y) with respect to x.\n",
        "y = y.mean()\n",
        "y.backward()\n",
        "print(x.grad)\n",
        "\n",
        "#if y.mean is not given then we have to use y.backward(torch.ones_like(y , dtype = torch.float32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DJ3RZ5nzLQU9",
        "outputId": "352ffa10-713d-4620-c807-418f915ffcf5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'x.requires_grad_(False)\\nx.detach()\\nwith torch.no_grad():'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#we can also deactivate the gradient tracking by using\n",
        "\"\"\"x.requires_grad_(False)\n",
        "x.detach()\n",
        "with torch.no_grad():\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4gYgpUzMeCZ"
      },
      "outputs": [],
      "source": [
        "# y.grad has all the grad stored in tensor format but requires_grad must be true for that object\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEvP8125Nz__"
      },
      "source": [
        "# ***BACKPROPAGATION***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcfaRU9LNo45",
        "outputId": "b1f477b6-3b33-4e75-dbf4-8f664e3e42d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n"
          ]
        }
      ],
      "source": [
        "#for simple linera regression\n",
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "w = torch.tensor(1.0 , requires_grad=True)\n",
        "\n",
        "#forward Pass\n",
        "y_pred = w * x\n",
        "loss = (y_pred - y)**2\n",
        "\n",
        "print(loss)\n",
        "#tensor(1., grad_fn=<PowBackward0>)\n",
        "\n",
        "#backward Pass\n",
        "loss.backward()\n",
        "print(w.grad)  #first gradient\n",
        "\n",
        "\n",
        "\n",
        "#update weight by using optimizers and optimizer.step() function\n",
        "\n",
        "##we wil do many such forward and backward pass until we get minimun loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYi6m954cFPj"
      },
      "source": [
        "# ***Gradient Descent Using AutoGrad***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBIUidpXcMCr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6dVCGr7ctoO"
      },
      "outputs": [],
      "source": [
        "#linear regression y = w * x\n",
        "\n",
        "x = torch.tensor([1,2,3,4] , dtype = torch.float32)\n",
        "y = torch.tensor([2,4,6,8] , dtype = torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E75D_5pqjzrh"
      },
      "outputs": [],
      "source": [
        "w = torch.tensor(0.0 , dtype = torch.float32 , requires_grad = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unjoq6HCdRJO",
        "outputId": "09cacaf9-b773-4957-f67a-d96d5de65ba9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction Before Training : f(2) =  0.000\n",
            "epoch 1 , w = 0.300 , loss = 30.00000000\n",
            "epoch 2 , w = 0.555 , loss = 21.67499924\n",
            "Prediction After Training : f(2) =  55499995217920.000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "#model prediction\n",
        "\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "#loss MSE\n",
        "\n",
        "def loss(y , y_pred):\n",
        "  return ((y_pred - y)**2).mean()\n",
        "\n",
        "\"\"\"#gradient\n",
        "#MSE = 1 / N * (loss)\n",
        "# dl / dw = 1/N 2x (w * x - y)\n",
        "def gradient(x , y , y_pred):\n",
        "  return np.dot(2*x , y_pred - y).mean()\"\"\"\n",
        "\n",
        "\n",
        "print(f\"Prediction Before Training : f(2) = {forward(2) : .3f}\")\n",
        "\n",
        "#training\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_iter = 2\n",
        "\n",
        "for epoch in range(n_iter):\n",
        "  y_pred = forward(x)\n",
        "  l = loss(y , y_pred)\n",
        "  #gradient = backward()\n",
        "  l.backward()\n",
        "  grad = w.grad\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #update our weights\n",
        "  with torch.no_grad():\n",
        "    # Use in-place operation to update w\n",
        "    w-= learning_rate * grad # Correct update\n",
        "\n",
        "  #reinitialize grad tp zero\n",
        "  grad.zero_()\n",
        "\n",
        "  print(f\"epoch {epoch + 1} , w = {w:.3f} , loss = {l:.8f}\")\n",
        "\n",
        "print(f\"Prediction After Training : f(2) = {forward(100000000000000) : .3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpiAUQEormos"
      },
      "source": [
        "# ***Loss & Optimizer Class in PyTorch***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMw5TanJrvyt"
      },
      "outputs": [],
      "source": [
        "#Training Pipeline\n",
        "\n",
        "# 1) Design model (input , output size , forward pass))\n",
        "# 2) Construct loss and optimizer\n",
        "# 3) Training lopp\n",
        "#   - Forward pass : Prediction\n",
        "#   - backward pass : gradient calaculation\n",
        "#   - update weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJsk__5asWKg"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rjp-aOwstLT",
        "outputId": "8f3763a1-05a1-4809-f68e-1243e0881bf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4 1\n",
            "Prediction Before Training : f(2) = -4.534\n",
            "epoch 1 , w = -0.390 , b = -0.219 , loss = 65.55545044\n",
            "epoch 2 , w = -0.020 , b = -0.095 , loss = 45.50182343\n",
            "epoch 3 , w = 0.287 , b = 0.008 , loss = 31.58695030\n",
            "epoch 4 , w = 0.544 , b = 0.093 , loss = 21.93162537\n",
            "epoch 5 , w = 0.758 , b = 0.164 , loss = 15.23191833\n",
            "epoch 6 , w = 0.936 , b = 0.223 , loss = 10.58304787\n",
            "epoch 7 , w = 1.084 , b = 0.272 , loss = 7.35721207\n",
            "epoch 8 , w = 1.208 , b = 0.312 , loss = 5.11879063\n",
            "epoch 9 , w = 1.311 , b = 0.345 , loss = 3.56551456\n",
            "epoch 10 , w = 1.397 , b = 0.373 , loss = 2.48764658\n",
            "Prediction After Training : f(5) =  7.359\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#linear regression y = w * x\n",
        "\n",
        "# Initialize input (x) and output (y) tensors\n",
        "# Tensors are shaped as (batch_size, input_features) and (batch_size, output_features)\n",
        "# to be compatible with PyTorch's nn.Linear module, which expects batched input.\n",
        "# Here, batch_size = 4 and input/output_features = 1.\n",
        "x = torch.tensor([[1],[2],[3],[4]] , dtype = torch.float32)\n",
        "y = torch.tensor([[2],[4],[6],[8]] , dtype = torch.float32)\n",
        "x_test = torch.tensor([5], dtype = torch.float32)\n",
        "n_samples , n_features = x.shape\n",
        "print(n_samples , n_features)\n",
        "\"\"\"\n",
        "here we do not need weights and forward to initialize because\n",
        "pytorch itself handle this\n",
        "w = torch.tensor(0.0 , dtype = torch.float32 , requires_grad = True)\n",
        "\n",
        "\n",
        "#model prediction\n",
        "\n",
        "def forward(x):\n",
        "  return w * x\"\"\"\n",
        "input_size , op_size = n_features , n_features\n",
        "#model = nn.Linear(input_size , op_size)\n",
        "\n",
        "# we can also design a model\n",
        "class LinearReg(nn.Module):\n",
        "  def __init__(self , input_dim  , output_dim):\n",
        "    super(LinearReg , self).__init__()\n",
        "    self.layer = nn.Linear(input_dim , output_dim)\n",
        "\n",
        "  def forward(self , x):\n",
        "    return self.layer(x)\n",
        "model = LinearReg(input_size , op_size)\n",
        "#loss MSE\n",
        "loss = nn.MSELoss()\n",
        "\"\"\"\n",
        "def loss(y , y_pred):\n",
        "  return ((y_pred - y)**2).mean()\"\"\"\n",
        "\n",
        "\"\"\"#gradient\n",
        "#MSE = 1 / N * (loss)\n",
        "# dl / dw = 1/N 2x (w * x - y)\n",
        "def gradient(x , y , y_pred):\n",
        "  return np.dot(2*x , y_pred - y).mean()\"\"\"\n",
        "\n",
        "\n",
        "print(f\"Prediction Before Training : f(2) = {model(x_test).item() : .3f}\")\n",
        "\n",
        "#training\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_iter = 10\n",
        "\n",
        "#for automatic weights update\n",
        "optimzer = optim.SGD(model.parameters() , lr=learning_rate)\n",
        "for epoch in range(n_iter):\n",
        "  y_pred = model(x)\n",
        "  l = loss(y , y_pred)\n",
        "  #gradient = backward()\n",
        "  l.backward()\n",
        "\n",
        "  #update wights\n",
        "  optimzer.step()\n",
        "  #reinitialize grad tp zero\n",
        "  optimzer.zero_grad()\n",
        "\n",
        "\n",
        "  [w , b] = model.parameters()\n",
        "  print(f\"epoch {epoch + 1} , w = {w.item():.3f} , b = {b.item():.3f} , loss = {l.item():.8f}\")\n",
        "\n",
        "print(f\"Prediction After Training : f(5) = {model(x_test).item() : .3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DV3tl5e7cj0"
      },
      "source": [
        "# ***Linear Regression***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "jLscVrm07gin",
        "outputId": "2162b406-cf4e-47f7-ad08-eaba3c81e5f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs : 10 , loss : 3094.6184\n",
            "Epochs : 20 , loss : 1632.5621\n",
            "Epochs : 30 , loss : 1083.9036\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "#Step 0 - Prepare data\n",
        "x_numpy , y_numpy = datasets.make_regression(n_samples=100 , n_features=1 , noise=30 , random_state=1)\n",
        "\n",
        "#convert it to tensor\n",
        "x = torch.from_numpy(x_numpy.astype(np.float32))\n",
        "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
        "y = y.view(y.shape[0] , 1)\n",
        "\n",
        "n_samples , n_features = x.shape\n",
        "\n",
        "#\n",
        "#Step 1 - Model Design\n",
        "\n",
        "\n",
        "class Linear(nn.Module):\n",
        "  def __init__(self , input_dim , output_dim):\n",
        "    super(Linear , self).__init__()\n",
        "    self.layer = nn.Linear(input_dim , output_dim)\n",
        "\n",
        "  def forward(self , x):\n",
        "    return self.layer(x)\n",
        "output_size = 1\n",
        "model = Linear(n_features , output_size)\n",
        "\n",
        "\n",
        "\n",
        "#Step 2 - Loss and Optimizer\n",
        "loss = nn.MSELoss()\n",
        "optimz = optim.SGD(model.parameters(), lr=0.03)\n",
        "\n",
        "\n",
        "#Step 3 - Training\n",
        "epochs = 30\n",
        "for epoch in range(epochs):\n",
        "  #FB\n",
        "  y_pred = model(x)\n",
        "\n",
        "  #loss\n",
        "  loss_ = loss(y_pred , y)\n",
        "\n",
        "  #BP\n",
        "  loss_.backward()\n",
        "\n",
        "  #update\n",
        "  optimz.step()\n",
        "\n",
        "  #making grad 0\n",
        "  optimz.zero_grad()\n",
        "\n",
        "  if (epoch+1)%10 == 0:\n",
        "    print(f\"Epochs : {epoch+1} , loss : {loss_.item():.4f}\")\n",
        "\n",
        "#plot\n",
        "predicted = model(x).detach()   #detach grad calculation\n",
        "plt.plot(x_numpy , y_numpy , 'ro')\n",
        "plt.plot(x_numpy , predicted , 'b')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puAuF8dZDbUQ"
      },
      "source": [
        "## ***Logistic Regression***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFeHSULdDayG",
        "outputId": "52801994-f86a-4f40-f51e-1220adb543b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs : 100 , loss : 0.2348\n",
            "Epochs : 200 , loss : 0.1704\n",
            "Epochs : 300 , loss : 0.1422\n",
            "Epochs : 400 , loss : 0.1254\n",
            "Epochs : 500 , loss : 0.1142\n",
            "Epochs : 600 , loss : 0.1059\n",
            "Epochs : 700 , loss : 0.0996\n",
            "Epochs : 800 , loss : 0.0945\n",
            "Epochs : 900 , loss : 0.0903\n",
            "Epochs : 1000 , loss : 0.0868\n",
            "Accuracy : 0.9386\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#Step 0 - Prepare data\n",
        "db = datasets.load_breast_cancer()\n",
        "x_numpy , y_numpy = db.data , db.target\n",
        "\n",
        "#train_test_split\n",
        "x_train , x_test , y_train , y_test = train_test_split(x_numpy , y_numpy , test_size=0.2 , random_state=1234)\n",
        "\n",
        "#Sacle out Features\n",
        "sc = StandardScaler()\n",
        "x_train = sc.fit_transform(x_train)\n",
        "x_test = sc.transform(x_test)\n",
        "\n",
        "\n",
        "#convert it to tensor\n",
        "x_train = torch.from_numpy(x_train.astype(np.float32))\n",
        "x_test = torch.from_numpy(x_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "y_train = y_train.view(y_train.shape[0] , 1)\n",
        "y_test = y_test.view(y_test.shape[0] , 1)\n",
        "\n",
        "n_samples , n_features = x_train.shape\n",
        "\n",
        "\n",
        "#Step 1 - Model Design\n",
        "\n",
        "# Logistic Regression Model\n",
        "class Log_Reg(nn.Module):\n",
        "  def __init__(self , input_dim , output_dim):\n",
        "    super(Log_Reg , self).__init__()\n",
        "    # Logistic Regression is a linear layer followed by a sigmoid activation\n",
        "    self.linear = nn.Linear(input_dim , output_dim)\n",
        "\n",
        "  def forward(self , x):\n",
        "    # Apply linear transformation and then sigmoid activation\n",
        "    return torch.sigmoid(self.linear(x))\n",
        "\n",
        "model = Log_Reg(n_features , 1)\n",
        "\n",
        "\n",
        "\n",
        "#Step 2 - Loss and Optimizer\n",
        "# BCELoss is used for binary classification with sigmoid output\n",
        "loss = nn.BCELoss()\n",
        "optimz = optim.SGD(model.parameters(), lr=0.01) # Reduced learning rate for stability\n",
        "\n",
        "\n",
        "#Step 3 - Training\n",
        "epochs = 1000 # Increased epochs for better convergence\n",
        "for epoch in range(epochs):\n",
        "  #FB\n",
        "  y_pred = model(x_train)\n",
        "\n",
        "  #loss\n",
        "  loss_ = loss(y_pred , y_train)\n",
        "\n",
        "  #BP\n",
        "  loss_.backward()\n",
        "\n",
        "  #update\n",
        "  optimz.step()\n",
        "\n",
        "  #making grad 0\n",
        "  optimz.zero_grad()\n",
        "\n",
        "  if (epoch+1)%100 == 0: # Print less frequently for many epochs\n",
        "    print(f\"Epochs : {epoch+1} , loss : {loss_.item():.4f}\")\n",
        "\n",
        "\n",
        "#Evaluation\n",
        "with torch.no_grad():\n",
        "  pred = model(x_test)\n",
        "  pred = pred.round() # Round predictions to 0 or 1\n",
        "  correct = (pred == y_test).sum().item() # Count correct predictions\n",
        "  acc = correct / y_test.shape[0] # Calculate accuracy\n",
        "  print(f\"Accuracy : {acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nctT0gHyKOcK"
      },
      "source": [
        "# ***DataSet and DataLoader***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "Kuf9LRsMKT5I",
        "outputId": "01e66b4e-16c6-498d-e190-bcbe489b9b89"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nUnderstanding Training Concepts: Epoch, Batch, and Iteration\\n\\nEpoch: One complete pass through the entire training dataset.\\nDuring one epoch, every training sample is used exactly once to update the model's weights.\\n\\nBatch (or Mini-Batch): A small subset of the training data that is used in one iteration\\nto compute the gradient and update the model's weights. Using batches is more computationally\\nefficient than using the entire dataset at once (batch gradient descent) and provides a\\nmore stable gradient estimate than using one sample at a time (stochastic gradient descent).\\n\\nIteration: One training step where a single batch of data is processed.\\nIn each iteration:\\n1. A batch of data is fed forward through the model to get predictions.\\n2. The loss is calculated based on the predictions and the true labels for that batch.\\n3. The gradients of the loss with respect to the model's parameters are computed (backward pass).\\n4. The optimizer uses these gradients to update the model's parameters.\\n\\nThe number of iterations in one epoch is equal to `total_number_of_training_samples / batch_size`.\\n\""
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Understanding Training Concepts: Epoch, Batch, and Iteration\n",
        "\n",
        "Epoch: One complete pass through the entire training dataset.\n",
        "During one epoch, every training sample is used exactly once to update the model's weights.\n",
        "\n",
        "Batch (or Mini-Batch): A small subset of the training data that is used in one iteration\n",
        "to compute the gradient and update the model's weights. Using batches is more computationally\n",
        "efficient than using the entire dataset at once (batch gradient descent) and provides a\n",
        "more stable gradient estimate than using one sample at a time (stochastic gradient descent).\n",
        "\n",
        "Iteration: One training step where a single batch of data is processed.\n",
        "In each iteration:\n",
        "1. A batch of data is fed forward through the model to get predictions.\n",
        "2. The loss is calculated based on the predictions and the true labels for that batch.\n",
        "3. The gradients of the loss with respect to the model's parameters are computed (backward pass).\n",
        "4. The optimizer uses these gradients to update the model's parameters.\n",
        "\n",
        "The number of iterations in one epoch is equal to `total_number_of_training_samples / batch_size`.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcbFMejtZeyH",
        "outputId": "1783e790-4555-41f2-f85b-21f3e7e74e14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-07-18 09:39:16--  https://archive.ics.uci.edu/static/public/109/wine.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: wine.zip\n",
            "\n",
            "wine.zip                [ <=>                ]   5.90K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-07-18 09:39:17 (108 MB/s) - wine.zip saved [6038]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://archive.ics.uci.edu/static/public/109/wine.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qpf7jS2KZi4J",
        "outputId": "4da04811-011a-423d-9ef9-71aab8bebd97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  wine.zip\n",
            "  inflating: Index                   \n",
            "  inflating: wine.data               \n",
            "  inflating: wine.names              \n"
          ]
        }
      ],
      "source": [
        "!unzip wine.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doOTNN_OZ12Z",
        "outputId": "9b2b0258-85ee-463e-d7b5-f76bb4cfbb60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Title of Database: Wine recognition data\n",
            "\tUpdated Sept 21, 1998 by C.Blake : Added attribute information\n",
            "\n",
            "2. Sources:\n",
            "   (a) Forina, M. et al, PARVUS - An Extendible Package for Data\n",
            "       Exploration, Classification and Correlation. Institute of Pharmaceutical\n",
            "       and Food Analysis and Technologies, Via Brigata Salerno, \n",
            "       16147 Genoa, Italy.\n",
            "\n",
            "   (b) Stefan Aeberhard, email: stefan@coral.cs.jcu.edu.au\n",
            "   (c) July 1991\n",
            "3. Past Usage:\n",
            "\n",
            "   (1)\n",
            "   S. Aeberhard, D. Coomans and O. de Vel,\n",
            "   Comparison of Classifiers in High Dimensional Settings,\n",
            "   Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of\n",
            "   Mathematics and Statistics, James Cook University of North Queensland.\n",
            "   (Also submitted to Technometrics).\n",
            "\n",
            "   The data was used with many others for comparing various \n",
            "   classifiers. The classes are separable, though only RDA \n",
            "   has achieved 100% correct classification.\n",
            "   (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data))\n",
            "   (All results using the leave-one-out technique)\n",
            "\n",
            "   In a classification context, this is a well posed problem \n",
            "   with \"well behaved\" class structures. A good data set \n",
            "   for first testing of a new classifier, but not very \n",
            "   challenging.\n",
            "\n",
            "   (2) \n",
            "   S. Aeberhard, D. Coomans and O. de Vel,\n",
            "   \"THE CLASSIFICATION PERFORMANCE OF RDA\"\n",
            "   Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of\n",
            "   Mathematics and Statistics, James Cook University of North Queensland.\n",
            "   (Also submitted to Journal of Chemometrics).\n",
            "\n",
            "   Here, the data was used to illustrate the superior performance of\n",
            "   the use of a new appreciation function with RDA. \n",
            "\n",
            "4. Relevant Information:\n",
            "\n",
            "   -- These data are the results of a chemical analysis of\n",
            "      wines grown in the same region in Italy but derived from three\n",
            "      different cultivars.\n",
            "      The analysis determined the quantities of 13 constituents\n",
            "      found in each of the three types of wines. \n",
            "\n",
            "   -- I think that the initial data set had around 30 variables, but \n",
            "      for some reason I only have the 13 dimensional version. \n",
            "      I had a list of what the 30 or so variables were, but a.) \n",
            "      I lost it, and b.), I would not know which 13 variables\n",
            "      are included in the set.\n",
            "\n",
            "   -- The attributes are (dontated by Riccardo Leardi, \n",
            "\triclea@anchem.unige.it )\n",
            " \t1) Alcohol\n",
            " \t2) Malic acid\n",
            " \t3) Ash\n",
            "\t4) Alcalinity of ash  \n",
            " \t5) Magnesium\n",
            "\t6) Total phenols\n",
            " \t7) Flavanoids\n",
            " \t8) Nonflavanoid phenols\n",
            " \t9) Proanthocyanins\n",
            "\t10)Color intensity\n",
            " \t11)Hue\n",
            " \t12)OD280/OD315 of diluted wines\n",
            " \t13)Proline            \n",
            "\n",
            "5. Number of Instances\n",
            "\n",
            "      \tclass 1 59\n",
            "\tclass 2 71\n",
            "\tclass 3 48\n",
            "\n",
            "6. Number of Attributes \n",
            "\t\n",
            "\t13\n",
            "\n",
            "7. For Each Attribute:\n",
            "\n",
            "\tAll attributes are continuous\n",
            "\t\n",
            "\tNo statistics available, but suggest to standardise\n",
            "\tvariables for certain uses (e.g. for us with classifiers\n",
            "\twhich are NOT scale invariant)\n",
            "\n",
            "\tNOTE: 1st attribute is class identifier (1-3)\n",
            "\n",
            "8. Missing Attribute Values:\n",
            "\n",
            "\tNone\n",
            "\n",
            "9. Class Distribution: number of instances per class\n",
            "\n",
            "      \tclass 1 59\n",
            "\tclass 2 71\n",
            "\tclass 3 48\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open(\"/content/wine.names\") as f:\n",
        "  print(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGThLFZiapAt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "wine_attributes = [\n",
        "    \"Class\", # Added the Class column\n",
        "    \"Alcohol\",\n",
        "    \"Malic acid\",\n",
        "    \"Ash\",\n",
        "    \"Alcalinity of ash\",\n",
        "    \"Magnesium\",\n",
        "    \"Total phenols\",\n",
        "    \"Flavanoids\",\n",
        "    \"Nonflavanoid phenols\",\n",
        "    \"Proanthocyanins\",\n",
        "    \"Color intensity\",\n",
        "    \"Hue\",\n",
        "    \"OD280/OD315 of diluted wines\",\n",
        "    \"Proline\"\n",
        "]\n",
        "data = pd.read_csv(\"/content/wine.data\" , header = None , sep = ',')\n",
        "data.columns = wine_attributes # Uncommented to set column names\n",
        "data = np.array(data) # Convert DataFrame to NumPy array immediately after loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMB3OFXDuLr8"
      },
      "outputs": [],
      "source": [
        "# Removed the data.describe() line as data is now a NumPy array\n",
        "# If you need descriptive statistics for the NumPy array, use NumPy functions like np.mean(), np.std(), etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f7b985f",
        "outputId": "efa210e9-ae31-4914-8d33-b40e5a64cad4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First data sample: tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
            "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
            "        1.0650e+03]), its label: tensor([1])\n",
            "178 45\n",
            "epoch 1/10 , step 5/45 , inputs torch.Size([4, 13])\n",
            "epoch 1/10 , step 10/45 , inputs torch.Size([4, 13])\n",
            "epoch 1/10 , step 15/45 , inputs torch.Size([4, 13])\n",
            "epoch 1/10 , step 20/45 , inputs torch.Size([4, 13])\n",
            "epoch 1/10 , step 25/45 , inputs torch.Size([4, 13])\n",
            "epoch 1/10 , step 30/45 , inputs torch.Size([4, 13])\n",
            "epoch 1/10 , step 35/45 , inputs torch.Size([4, 13])\n",
            "epoch 1/10 , step 40/45 , inputs torch.Size([4, 13])\n",
            "epoch 1/10 , step 45/45 , inputs torch.Size([2, 13])\n",
            "epoch 2/10 , step 5/45 , inputs torch.Size([4, 13])\n",
            "epoch 2/10 , step 10/45 , inputs torch.Size([4, 13])\n",
            "epoch 2/10 , step 15/45 , inputs torch.Size([4, 13])\n",
            "epoch 2/10 , step 20/45 , inputs torch.Size([4, 13])\n",
            "epoch 2/10 , step 25/45 , inputs torch.Size([4, 13])\n",
            "epoch 2/10 , step 30/45 , inputs torch.Size([4, 13])\n",
            "epoch 2/10 , step 35/45 , inputs torch.Size([4, 13])\n",
            "epoch 2/10 , step 40/45 , inputs torch.Size([4, 13])\n",
            "epoch 2/10 , step 45/45 , inputs torch.Size([2, 13])\n",
            "epoch 3/10 , step 5/45 , inputs torch.Size([4, 13])\n",
            "epoch 3/10 , step 10/45 , inputs torch.Size([4, 13])\n",
            "epoch 3/10 , step 15/45 , inputs torch.Size([4, 13])\n",
            "epoch 3/10 , step 20/45 , inputs torch.Size([4, 13])\n",
            "epoch 3/10 , step 25/45 , inputs torch.Size([4, 13])\n",
            "epoch 3/10 , step 30/45 , inputs torch.Size([4, 13])\n",
            "epoch 3/10 , step 35/45 , inputs torch.Size([4, 13])\n",
            "epoch 3/10 , step 40/45 , inputs torch.Size([4, 13])\n",
            "epoch 3/10 , step 45/45 , inputs torch.Size([2, 13])\n",
            "epoch 4/10 , step 5/45 , inputs torch.Size([4, 13])\n",
            "epoch 4/10 , step 10/45 , inputs torch.Size([4, 13])\n",
            "epoch 4/10 , step 15/45 , inputs torch.Size([4, 13])\n",
            "epoch 4/10 , step 20/45 , inputs torch.Size([4, 13])\n",
            "epoch 4/10 , step 25/45 , inputs torch.Size([4, 13])\n",
            "epoch 4/10 , step 30/45 , inputs torch.Size([4, 13])\n",
            "epoch 4/10 , step 35/45 , inputs torch.Size([4, 13])\n",
            "epoch 4/10 , step 40/45 , inputs torch.Size([4, 13])\n",
            "epoch 4/10 , step 45/45 , inputs torch.Size([2, 13])\n",
            "epoch 5/10 , step 5/45 , inputs torch.Size([4, 13])\n",
            "epoch 5/10 , step 10/45 , inputs torch.Size([4, 13])\n",
            "epoch 5/10 , step 15/45 , inputs torch.Size([4, 13])\n",
            "epoch 5/10 , step 20/45 , inputs torch.Size([4, 13])\n",
            "epoch 5/10 , step 25/45 , inputs torch.Size([4, 13])\n",
            "epoch 5/10 , step 30/45 , inputs torch.Size([4, 13])\n",
            "epoch 5/10 , step 35/45 , inputs torch.Size([4, 13])\n",
            "epoch 5/10 , step 40/45 , inputs torch.Size([4, 13])\n",
            "epoch 5/10 , step 45/45 , inputs torch.Size([2, 13])\n",
            "epoch 6/10 , step 5/45 , inputs torch.Size([4, 13])\n",
            "epoch 6/10 , step 10/45 , inputs torch.Size([4, 13])\n",
            "epoch 6/10 , step 15/45 , inputs torch.Size([4, 13])\n",
            "epoch 6/10 , step 20/45 , inputs torch.Size([4, 13])\n",
            "epoch 6/10 , step 25/45 , inputs torch.Size([4, 13])\n",
            "epoch 6/10 , step 30/45 , inputs torch.Size([4, 13])\n",
            "epoch 6/10 , step 35/45 , inputs torch.Size([4, 13])\n",
            "epoch 6/10 , step 40/45 , inputs torch.Size([4, 13])\n",
            "epoch 6/10 , step 45/45 , inputs torch.Size([2, 13])\n",
            "epoch 7/10 , step 5/45 , inputs torch.Size([4, 13])\n",
            "epoch 7/10 , step 10/45 , inputs torch.Size([4, 13])\n",
            "epoch 7/10 , step 15/45 , inputs torch.Size([4, 13])\n",
            "epoch 7/10 , step 20/45 , inputs torch.Size([4, 13])\n",
            "epoch 7/10 , step 25/45 , inputs torch.Size([4, 13])\n",
            "epoch 7/10 , step 30/45 , inputs torch.Size([4, 13])\n",
            "epoch 7/10 , step 35/45 , inputs torch.Size([4, 13])\n",
            "epoch 7/10 , step 40/45 , inputs torch.Size([4, 13])\n",
            "epoch 7/10 , step 45/45 , inputs torch.Size([2, 13])\n",
            "epoch 8/10 , step 5/45 , inputs torch.Size([4, 13])\n",
            "epoch 8/10 , step 10/45 , inputs torch.Size([4, 13])\n",
            "epoch 8/10 , step 15/45 , inputs torch.Size([4, 13])\n",
            "epoch 8/10 , step 20/45 , inputs torch.Size([4, 13])\n",
            "epoch 8/10 , step 25/45 , inputs torch.Size([4, 13])\n",
            "epoch 8/10 , step 30/45 , inputs torch.Size([4, 13])\n",
            "epoch 8/10 , step 35/45 , inputs torch.Size([4, 13])\n",
            "epoch 8/10 , step 40/45 , inputs torch.Size([4, 13])\n",
            "epoch 8/10 , step 45/45 , inputs torch.Size([2, 13])\n",
            "epoch 9/10 , step 5/45 , inputs torch.Size([4, 13])\n",
            "epoch 9/10 , step 10/45 , inputs torch.Size([4, 13])\n",
            "epoch 9/10 , step 15/45 , inputs torch.Size([4, 13])\n",
            "epoch 9/10 , step 20/45 , inputs torch.Size([4, 13])\n",
            "epoch 9/10 , step 25/45 , inputs torch.Size([4, 13])\n",
            "epoch 9/10 , step 30/45 , inputs torch.Size([4, 13])\n",
            "epoch 9/10 , step 35/45 , inputs torch.Size([4, 13])\n",
            "epoch 9/10 , step 40/45 , inputs torch.Size([4, 13])\n",
            "epoch 9/10 , step 45/45 , inputs torch.Size([2, 13])\n",
            "epoch 10/10 , step 5/45 , inputs torch.Size([4, 13])\n",
            "epoch 10/10 , step 10/45 , inputs torch.Size([4, 13])\n",
            "epoch 10/10 , step 15/45 , inputs torch.Size([4, 13])\n",
            "epoch 10/10 , step 20/45 , inputs torch.Size([4, 13])\n",
            "epoch 10/10 , step 25/45 , inputs torch.Size([4, 13])\n",
            "epoch 10/10 , step 30/45 , inputs torch.Size([4, 13])\n",
            "epoch 10/10 , step 35/45 , inputs torch.Size([4, 13])\n",
            "epoch 10/10 , step 40/45 , inputs torch.Size([4, 13])\n",
            "epoch 10/10 , step 45/45 , inputs torch.Size([2, 13])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset , DataLoader\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class WineDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    # Assuming the first column is the Class (label) and the rest are features\n",
        "    self.x = torch.from_numpy(data[:,1:]).type(torch.float32) # Ensure float32 type\n",
        "    self.y = torch.from_numpy(data[:,[0]]).type(torch.long) # Ensure long type for classification label\n",
        "    self.n_samples = data.shape[0]\n",
        "\n",
        "  def __getitem__(self , index):\n",
        "    #dataset[0]\n",
        "    return self.x[index] , self.y[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    #len(dataset)\n",
        "    return self.n_samples\n",
        "\n",
        "dataset = WineDataset()\n",
        "# First data sample and its corresponding label\n",
        "first_data, first_label = dataset[0]\n",
        "print(f\"First data sample: {first_data}, its label: {first_label}\")\n",
        "\n",
        "\n",
        "dataloader = DataLoader(dataset = dataset , batch_size = 4 , shuffle = True , num_workers = 2)\n",
        "\n",
        "\"\"\"# Get one batch of data from the DataLoader using the built-in next() function\n",
        "dataiter = iter(dataloader)\n",
        "features , labels = next(dataiter) # Changed from dataiter.next() to next(dataiter)\n",
        "print(f\"One batch of features: {features.shape}, labels: {labels.shape}\")\n",
        "print(features , labels)\"\"\"\n",
        "\n",
        "#Dummy Training Loop\n",
        "num_epoch = 10\n",
        "total_samples = len(dataset)\n",
        "n_iter = math.ceil(total_samples / 4)\n",
        "print(total_samples , n_iter)\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "  for i , (inputs , labels) in enumerate(dataloader):\n",
        "    if (i+1)%5 == 0:\n",
        "      print(f\"epoch {epoch+1}/{num_epoch} , step {i+1}/{n_iter} , inputs {inputs.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk-1l-mqynga"
      },
      "source": [
        "# ***Data Transforms***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cGyctyrytjv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dbae18d"
      },
      "outputs": [],
      "source": [
        "# PyTorch provides a variety of transforms to preprocess data for training.\n",
        "# These transforms can be applied to the input data (features) and/or the target labels.\n",
        "\n",
        "# Common Transforms for Images:\n",
        "# - Resize: Resize the image to a specified size.\n",
        "#   - Input: PIL Image, Tensor Image\n",
        "#   - Output: PIL Image, Tensor Image\n",
        "# - RandomCrop: Crop a random portion of the image.\n",
        "#   - Input: PIL Image, Tensor Image\n",
        "#   - Output: PIL Image, Tensor Image\n",
        "# - CenterCrop: Crop the center portion of the image.\n",
        "#   - Input: PIL Image, Tensor Image\n",
        "#   - Output: PIL Image, Tensor Image\n",
        "# - RandomHorizontalFlip: Randomly flip the image horizontally.\n",
        "#   - Input: PIL Image, Tensor Image\n",
        "#   - Output: PIL Image, Tensor Image\n",
        "# - RandomVerticalFlip: Randomly flip the image vertically.\n",
        "#   - Input: PIL Image, Tensor Image\n",
        "#   - Output: PIL Image, Tensor Image\n",
        "# - ToTensor: Convert a PIL Image or NumPy array to a PyTorch Tensor.\n",
        "#   - Input: PIL Image, numpy.ndarray\n",
        "#   - Output: torch.Tensor\n",
        "# - Normalize: Normalize a tensor image with mean and standard deviation.\n",
        "#   - Input: torch.Tensor\n",
        "#   - Output: torch.Tensor\n",
        "# - Compose: Compose multiple transforms together.\n",
        "#   - Input: Varies depending on the first transform in the sequence\n",
        "#   - Output: Varies depending on the last transform in the sequence\n",
        "\n",
        "# Common Transforms for other data types (e.g., text, audio):\n",
        "# - These transforms are often more dataset-specific and might involve:\n",
        "# - Tokenization: Breaking down text into words or sub-word units.\n",
        "#   - Input: String\n",
        "#   - Output: List of strings or tokens\n",
        "# - Numericalization: Converting tokens into numerical representations.\n",
        "#   - Input: List of strings or tokens\n",
        "#   - Output: List or Tensor of integers (indices)\n",
        "# - Padding: Adding padding to sequences to make them of equal length.\n",
        "#   - Input: List or Tensor\n",
        "#   - Output: Tensor\n",
        "# - Feature scaling: Standardizing or normalizing numerical features.\n",
        "#   - Input: numpy.ndarray or torch.Tensor\n",
        "#   - Output: numpy.ndarray or torch.Tensor\n",
        "\n",
        "# Custom Transforms:\n",
        "# - You can also create your own custom transforms by defining a class\n",
        "#   that implements the __call__ method. This method takes the data as input\n",
        "#   and returns the transformed data.\n",
        "#   - Input: Varies depending on the custom transform\n",
        "#   - Output: Varies depending on the custom transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUyfze066PaY",
        "outputId": "68ead800-e80f-46de-b351-1d626cedf9d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([  19.2300,    6.7100,    7.4300,   20.6000,  132.0000,    7.8000,\n",
            "           8.0600,    5.2800,    7.2900,   10.6400,    6.0400,    8.9200,\n",
            "        1070.0000], dtype=torch.float64) tensor([1.], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "class WineDataset(Dataset):\n",
        "  def __init__(self , transform = None):\n",
        "    # Assuming the first column is the Class (label) and the rest are features\n",
        "    self.x = data[:,1:]\n",
        "    self.y = data[:,[0]]  #only np arrays\n",
        "    self.n_samples = data.shape[0]\n",
        "\n",
        "\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self , index):\n",
        "    #dataset[0]\n",
        "    samples =  self.x[index] , self.y[index]\n",
        "    if self.transform:\n",
        "      return self.transform(samples)\n",
        "    return samples\n",
        "\n",
        "  def __len__(self):\n",
        "    #len(dataset)\n",
        "    return self.n_samples\n",
        "\n",
        "#Custom Transforms\n",
        "\n",
        "class toTensor:\n",
        "  def __call__(self , samples):\n",
        "    input , target = samples\n",
        "    return torch.from_numpy(input) , torch.from_numpy(target)\n",
        "\n",
        "class addTransform:\n",
        "  def __init__(self , number):\n",
        "    self.number = number\n",
        "\n",
        "  def __call__(self , samples):\n",
        "    inputs , target = samples\n",
        "    inputs += self.number\n",
        "    return inputs , target\n",
        "\n",
        "\n",
        "#for calling multiple transform we will use\n",
        "#torchvision.transform.Composed([list of transforms])\n",
        "composed = torchvision.transforms.Compose([toTensor() , addTransform(5)])\n",
        "dataset = WineDataset(transform = composed)\n",
        "first_data , first_label = dataset[0]\n",
        "print(first_data , first_label)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_7meQ6uw0uN"
      },
      "source": [
        "# ***SoftMax and CrossEntropy loss***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxxHWAq3w8f0",
        "outputId": "44c67c5e-c7c9-4f37-cc00-fec08ada2c57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Softmax Outputs : [0.65900114 0.24243297 0.09856589]\n",
            "Softmax Outputs : tensor([0.6590, 0.2424, 0.0986])\n"
          ]
        }
      ],
      "source": [
        "#Softmax activation\n",
        "#S(x) = e^x / summation of e^x --- Give multiple probabilities\n",
        "\n",
        "def softmax(x):\n",
        "  return np.exp(x) / np.sum(np.exp(x) , axis = 0)\n",
        "\n",
        "x = np.array([2.0 , 1.0 , 0.1])\n",
        "outputs = softmax(x)\n",
        "print(f\"Softmax Outputs : {outputs}\")\n",
        "\n",
        "x = torch.tensor([2.0 , 1.0 , 0.1])\n",
        "outputs = torch.softmax(x , dim = 0)\n",
        "print(f\"Softmax Outputs : {outputs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5-b4Tg9x1us",
        "outputId": "4e722a26-7a5a-435b-d835-00bbf436d36c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss 1 : 0.3567\n",
            "Loss 2 : 2.3026\n",
            "Loss 1 : 0.4170299470424652\n",
            "Loss 2 : 2.3170299530029297\n",
            "Loss 3 : 0.14350222051143646\n",
            "Loss 4 : 7.12270975112915\n",
            "tensor([0])\n",
            "tensor([2])\n",
            "tensor([0, 1, 2])\n",
            "tensor([2, 0, 1])\n"
          ]
        }
      ],
      "source": [
        "#Cross Entropy Loss\n",
        "# D(y_pred , y) = -1/N sum(y . log(y_pred))\n",
        "\n",
        "def cross_entropy(predicted , actual):\n",
        "  loss = -np.sum(actual * np.log(predicted))\n",
        "  return loss #/  [predicted.shape[0]]\n",
        "\n",
        "y = np.array([1 , 0 , 0])  #one hot encoded\n",
        "\n",
        "\n",
        "\n",
        "y_pred_good = np.array([0.7 , 0.2 , 0.1])\n",
        "y_pred_bad = np.array([0.1 , 0.3 , 0.6])\n",
        "l1 = cross_entropy(y_pred_good , y)\n",
        "l2 = cross_entropy(y_pred_bad , y)\n",
        "print(f\"Loss 1 : {l1:.4f}\")\n",
        "print(f\"Loss 2 : {l2:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "#In tensor\n",
        "#when we us this function we are required to not to use any softmax at last layer\n",
        "#and also we do not need to give in one ot encoded format\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "y = torch.tensor([0]) #only class 0\n",
        "#we can also do for multiple samples\n",
        "y_ = torch.tensor([0 , 1 , 2])\n",
        "#n_sample x n_classes = 3x1\n",
        "y_pred_good = torch.tensor([[2.0 , 1.0 , 0.1]])\n",
        "y_pred_bad = torch.tensor([[0.1 , 1.0 , 2.0]])\n",
        "y_pred_good_ = torch.tensor([[2.0 , 1.0 , 0.1] , [0 , 5 , 0] , [0 , 0 , 10]])\n",
        "y_pred_bad_ = torch.tensor([[0.1 , 0.2 , 0.7] , [10 , 0 , 0] , [0 , 10 , 0]])\n",
        "\n",
        "\n",
        "l1 = loss(y_pred_good , y)\n",
        "l2 = loss(y_pred_bad , y)\n",
        "print(f\"Loss 1 : {l1.item()}\")\n",
        "print(f\"Loss 2 : {l2.item()}\")\n",
        "\n",
        "l3 = loss(y_pred_good_ , y_)\n",
        "l4 = loss(y_pred_bad_ , y_)\n",
        "print(f\"Loss 3 : {l3.item()}\")\n",
        "print(f\"Loss 4 : {l4.item()}\")\n",
        "\n",
        "_ ,prediction1 = torch.max(y_pred_good , 1)\n",
        "\n",
        "_ ,prediction12 = torch.max(y_pred_bad , 1)\n",
        "\n",
        "_ , prediction2 = torch.max(y_pred_good_ , 1)\n",
        "\n",
        "_ , prediction3 = torch.max(y_pred_bad_ , 1)\n",
        "\n",
        "print(prediction1)\n",
        "print(prediction12)\n",
        "\n",
        "print(prediction2)\n",
        "print(prediction3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmyI1Jq177MA"
      },
      "source": [
        "# ***Activation Functions***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp-X5XfG5OjF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#option 1 (create nn module)\n",
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self , input_size , hidden_size):\n",
        "    super(NeuralNet , self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size , hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 =nn.Linear(hidden_size , 1)\n",
        "    \"\"\"nn.Sigmoid()\n",
        "    nn.Tanh()\n",
        "    nn.LeakyReLU()\n",
        "\"\"\"\n",
        "  def forward(self , x):\n",
        "    out = self.linear1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear2(out)\n",
        "    #sigmoid at the end\n",
        "    y_pred = torch.sigmoid(out)\n",
        "    return y_pred\n",
        "\n",
        "#option 2 (use activation functions directly in forward pass)\n",
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self , input_size , hidden_size):\n",
        "    super(NeuralNet , self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size , hidden_size)\n",
        "    self.linear2 =nn.Linear(hidden_size , 1)\n",
        "    \"\"\"\n",
        "    torch.relu()\n",
        "    torch.tanh\"\"\"\n",
        "\n",
        "  def forward(self , x):\n",
        "    out = torch.relu(self.linear1(x))\n",
        "    out = torch.sigmoid(self.linear2(out))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9abK5IxKAWFJ"
      },
      "source": [
        "# ***Feed Forward Neural network***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTdr2P3cAcDy",
        "outputId": "d25377be-d339-49be-f1f3-cd62c128eaef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 9.91M/9.91M [00:02<00:00, 4.87MB/s]\n",
            "100%|| 28.9k/28.9k [00:00<00:00, 133kB/s]\n",
            "100%|| 1.65M/1.65M [00:06<00:00, 241kB/s]\n",
            "100%|| 4.54k/4.54k [00:00<00:00, 8.29MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#MNIST Dataset\n",
        "#DalaLoader and Transformation\n",
        "#Multilayer neural net , activation function\n",
        "#loss and optimzers\n",
        "#training loop (batch training)\n",
        "#model evaluation\n",
        "#gpu support\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#device config\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "#hyperParameters\n",
        "\n",
        "input_size = 784  #28x28\n",
        "hidden_size = 100\n",
        "num_class = 10\n",
        "epochs = 2\n",
        "batch_size = 100\n",
        "lr = 0.001\n",
        "\n",
        "#MNIST\n",
        "\n",
        "dataset = torchvision.datasets.MNIST(root = './data' , train = True ,\n",
        "                                    transform = transforms.ToTensor() , download = True\n",
        "                                    )\n",
        "test_data = torchvision.datasets.MNIST(root = './data' , train = False ,\n",
        "                                    transform = transforms.ToTensor() , download = False\n",
        "                                    )\n",
        "train_loader = torch.utils.data.DataLoader(dataset = dataset , batch_size = batch_size , shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_data , batch_size = batch_size , shuffle = True)\n",
        "\n",
        "examples = iter(train_loader)\n",
        "samples , labels = next(examples)\n",
        "print(samples.shape , labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WYy0CT_Nbc4u",
        "outputId": "e9671693-11cd-4a56-b149-026c3a885210"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.imshow(samples[i][0] , cmap = 'gray')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fv6TwJQpbspc"
      },
      "outputs": [],
      "source": [
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self , input_size , hidden_size , num_class):\n",
        "    super(NeuralNet , self).__init__()\n",
        "    self.l1 = nn.Linear(input_size , hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.l2 = nn.Linear(hidden_size , num_class)\n",
        "\n",
        "  def forward(self , x):\n",
        "    out = self.l1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.l2(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0C3zxMKcZI1",
        "outputId": "2e5fdd3b-3748-4a98-ae7d-a889f65843c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/2], Step [100/600], Loss: 0.5023\n",
            "Epoch [1/2], Step [200/600], Loss: 0.3962\n",
            "Epoch [1/2], Step [300/600], Loss: 0.3046\n",
            "Epoch [1/2], Step [400/600], Loss: 0.1791\n",
            "Epoch [1/2], Step [500/600], Loss: 0.3375\n",
            "Epoch [1/2], Step [600/600], Loss: 0.2199\n",
            "Epoch [2/2], Step [100/600], Loss: 0.2059\n",
            "Epoch [2/2], Step [200/600], Loss: 0.2132\n",
            "Epoch [2/2], Step [300/600], Loss: 0.2431\n",
            "Epoch [2/2], Step [400/600], Loss: 0.1335\n",
            "Epoch [2/2], Step [500/600], Loss: 0.1321\n",
            "Epoch [2/2], Step [600/600], Loss: 0.2631\n",
            "Accuracy = 95.37\n"
          ]
        }
      ],
      "source": [
        "model = NeuralNet(input_size , hidden_size , num_class).to(device)\n",
        "\n",
        "#loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optim = torch.optim.Adam(model.parameters() , lr = lr)\n",
        "\n",
        "#training loop\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(epochs):\n",
        "  for i , (image , labels) in enumerate(train_loader):\n",
        "    #100,1,28,28\n",
        "    #needing - 100 , 784\n",
        "    image = image.reshape(-1,28*28).to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    #FP\n",
        "    output = model(image)\n",
        "    loss = criterion(output , labels)\n",
        "\n",
        "    #BP\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    if (i+1) % 100 == 0:\n",
        "      print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "#testing and evaluation\n",
        "with torch.no_grad():\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "  for images , labels in test_loader:\n",
        "    images = images.reshape(-1,28*28).to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(images)\n",
        "    _, predict = torch.max(output , 1)\n",
        "    n_samples += labels.shape[0]\n",
        "    n_correct += (predict == labels).sum().item()\n",
        "\n",
        "  acc = 100.0 * n_correct / n_samples\n",
        "  print(f'Accuracy = {acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdnwBfgJgpH4"
      },
      "source": [
        "# ***Convolutional Neural Network (CNN)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24a93244"
      },
      "outputs": [],
      "source": [
        "# Convolutional Neural Networks (CNNs)\n",
        "\n",
        "# What is a CNN?\n",
        "# A Convolutional Neural Network (CNN) is a type of artificial neural network specifically designed\n",
        "# for processing structured grid data, such as images, where patterns are spatially related.\n",
        "# They are highly effective in tasks like image classification, object detection, and image segmentation.\n",
        "\n",
        "# Key Components of a CNN:\n",
        "# 1. Convolutional Layer: This layer applies convolutional filters (kernels) to the input data\n",
        "#    to detect features like edges, corners, and textures. Each filter slides over the input,\n",
        "#    performing element-wise multiplications and summing the results to create a feature map.\n",
        "#    Multiple filters are typically used to extract various features.\n",
        "\n",
        "# 2. Activation Function: Applied after the convolutional layer (or sometimes other layers)\n",
        "#    to introduce non-linearity into the model. Common activation functions include ReLU (Rectified Linear Unit),\n",
        "#    Sigmoid, and Tanh. ReLU is widely used due to its simplicity and effectiveness in preventing vanishing gradients.\n",
        "\n",
        "# 3. Pooling Layer (or Downsampling Layer): This layer reduces the spatial dimensions (width and height)\n",
        "#    of the feature maps, which helps to decrease the computational cost, reduce overfitting, and\n",
        "#    make the network more robust to small variations in the input. Common pooling methods are\n",
        "#    Max Pooling and Average Pooling.\n",
        "\n",
        "# 4. Fully Connected Layer (Dense Layer): After several convolutional and pooling layers, the\n",
        "#    flattened feature maps are connected to one or more fully connected layers. These layers\n",
        "#    perform high-level reasoning based on the features extracted by the previous layers.\n",
        "\n",
        "# 5. Output Layer: The final fully connected layer, which outputs the prediction. For classification\n",
        "#    tasks, this layer often uses a Softmax activation function to output class probabilities.\n",
        "\n",
        "# How CNNs work in PyTorch:\n",
        "\n",
        "# PyTorch provides the `torch.nn` module to build neural networks, including CNNs.\n",
        "\n",
        "# Convolutional Layers:\n",
        "# - `torch.nn.Conv2d`: Used for 2D convolutions, typically for images.\n",
        "#   - Parameters include `in_channels` (number of input channels, e.g., 3 for RGB images),\n",
        "#     `out_channels` (number of output channels/filters), `kernel_size` (size of the convolutional filter),\n",
        "#     `stride` (step size of the filter), and `padding` (padding added to the input).\n",
        "\n",
        "# Activation Functions:\n",
        "# - `torch.nn.ReLU()`, `torch.nn.Sigmoid()`, `torch.nn.Tanh()`, etc.\n",
        "# - These can be added as separate layers or used directly as functions (`F.relu()`, `F.sigmoid()`, etc.)\n",
        "#   from `torch.nn.functional`.\n",
        "\n",
        "# Pooling Layers:\n",
        "# - `torch.nn.MaxPool2d()`: Used for 2D max pooling.\n",
        "#   - Parameters include `kernel_size` and `stride`.\n",
        "# - `torch.nn.AvgPool2d()`: Used for 2D average pooling.\n",
        "\n",
        "# Fully Connected Layers:\n",
        "# - `torch.nn.Linear()`: Used for linear transformations (the core of fully connected layers).\n",
        "#   - Parameters include `in_features` and `out_features`.\n",
        "\n",
        "# Building a CNN in PyTorch:\n",
        "# - You typically define a class that inherits from `torch.nn.Module`.\n",
        "# - In the `__init__` method, you define the layers (Conv2d, ReLU, MaxPool2d, Linear, etc.).\n",
        "# - In the `forward` method, you define the forward pass, specifying how data flows through the layers.\n",
        "#   You apply the layers and activation functions in the desired order.\n",
        "#   Remember to flatten the output of the convolutional/pooling layers before feeding it into the fully connected layers (often using `.view()` or `.flatten()`).\n",
        "\n",
        "# Example structure in PyTorch:\n",
        "# class SimpleCNN(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(SimpleCNN, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "#         self.fc = nn.Linear(16 * ...) # Calculate the correct input size based on pooling\n",
        "#         self.output = nn.Linear(..., num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.conv1(x)\n",
        "#         out = self.relu(out)\n",
        "#         out = self.pool(out)\n",
        "#         # Flatten the output for the fully connected layer\n",
        "#         out = out.view(out.size(0), -1)\n",
        "#         out = self.fc(out)\n",
        "#         out = self.output(out) # Apply softmax if needed for multi-class classification\n",
        "#         return out\n",
        "\n",
        "# Loss Function and Optimizer:\n",
        "# - Similar to other neural networks, you choose an appropriate loss function\n",
        "#   (e.g., `nn.CrossEntropyLoss` for multi-class classification) and an optimizer\n",
        "#   (e.g., `optim.Adam`, `optim.SGD`) to train the CNN.\n",
        "\n",
        "# Training Loop:\n",
        "# - The training loop involves iterating through the data in batches, performing\n",
        "#   the forward pass, calculating the loss, performing the backward pass\n",
        "#   (gradient calculation), and updating the weights using the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRg0xwSuhVBR",
        "outputId": "0b2fc995-97f8-41f0-dd86-c668948593fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 170M/170M [00:13<00:00, 12.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "#CIFAR - 10 dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#device config\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "#hyperParameters\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 4\n",
        "lr = 0.001\n",
        "\n",
        "#CIFAR - 10\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "dataset = torchvision.datasets.CIFAR10(root = './data' , train = True ,\n",
        "                                    transform = transform , download = True\n",
        "                                    )\n",
        "test_data = torchvision.datasets.CIFAR10(root = './data' , train = False ,\n",
        "                                    transform = transform , download = False\n",
        "                                    )\n",
        "train_loader = torch.utils.data.DataLoader(dataset = dataset , batch_size = batch_size , shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_data , batch_size = batch_size , shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b96FgiQbjQFd"
      },
      "outputs": [],
      "source": [
        "classes = ('plane' , 'car' , 'bird' , 'cat' , 'deer' , 'dog' , 'frog' , 'horse' , 'ship' , 'truck')\n",
        "\n",
        "# Define a simple Convolutional Neural Network\n",
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvNet , self).__init__()\n",
        "    # First convolutional layer: 3 input channels (RGB), 6 output channels, 5x5 kernel\n",
        "    self.l1 = nn.Conv2d(3 , 6 , 5)\n",
        "    # Max pooling layer: 2x2 window, stride of 2\n",
        "    self.pool = nn.MaxPool2d(2 , 2)\n",
        "    # Second convolutional layer: 6 input channels (from previous layer), 16 output channels, 5x5 kernel\n",
        "    self.l2 = nn.Conv2d(6 , 16 , 5)\n",
        "    # First fully connected layer: 16*5*5 input features (flattened output from conv/pool), 120 output features\n",
        "    self.fc1 = nn.Linear(16*5*5 , 120)\n",
        "    # Second fully connected layer: 120 input features, 84 output features\n",
        "    self.fc2 = nn.Linear(120 , 84)\n",
        "    # Output fully connected layer: 84 input features, 10 output features (number of classes)\n",
        "    self.fc3 = nn.Linear(84 , 10)\n",
        "\n",
        "  def forward(self , x):\n",
        "    x = self.pool(F.relu(self.l1(x)))\n",
        "    x = self.pool(F.relu(self.l2(x)))\n",
        "    x = x.view(-1 , 16*5*5)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "model = ConvNet().to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6tZeGGZjfkG",
        "outputId": "427ba0b9-32ec-48dc-8a5e-076b75cefdd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Step [10000/12500], Loss: 1.9317\n",
            "Epoch [2/20], Step [10000/12500], Loss: 1.3138\n",
            "Epoch [3/20], Step [10000/12500], Loss: 0.9152\n",
            "Epoch [4/20], Step [10000/12500], Loss: 0.8227\n",
            "Epoch [5/20], Step [10000/12500], Loss: 1.1025\n",
            "Epoch [6/20], Step [10000/12500], Loss: 1.4542\n",
            "Epoch [7/20], Step [10000/12500], Loss: 1.1655\n",
            "Epoch [8/20], Step [10000/12500], Loss: 0.9894\n",
            "Epoch [9/20], Step [10000/12500], Loss: 0.4498\n",
            "Epoch [10/20], Step [10000/12500], Loss: 1.0457\n",
            "Epoch [11/20], Step [10000/12500], Loss: 0.3506\n",
            "Epoch [12/20], Step [10000/12500], Loss: 0.4363\n",
            "Epoch [13/20], Step [10000/12500], Loss: 1.3679\n",
            "Epoch [14/20], Step [10000/12500], Loss: 0.4172\n",
            "Epoch [15/20], Step [10000/12500], Loss: 1.2105\n",
            "Epoch [16/20], Step [10000/12500], Loss: 1.0806\n",
            "Epoch [17/20], Step [10000/12500], Loss: 0.9577\n",
            "Epoch [18/20], Step [10000/12500], Loss: 0.5992\n",
            "Epoch [19/20], Step [10000/12500], Loss: 0.2295\n",
            "Epoch [20/20], Step [10000/12500], Loss: 1.3925\n",
            "Accuracy = 61.78\n",
            "Accuracy of plane : 71.0\n",
            "Accuracy of car : 73.6\n",
            "Accuracy of bird : 43.1\n",
            "Accuracy of cat : 53.2\n",
            "Accuracy of deer : 58.1\n",
            "Accuracy of dog : 49.4\n",
            "Accuracy of frog : 68.2\n",
            "Accuracy of horse : 65.9\n",
            "Accuracy of ship : 68.0\n",
            "Accuracy of truck : 67.3\n"
          ]
        }
      ],
      "source": [
        "#loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optim = torch.optim.Adam(model.parameters() , lr = lr)\n",
        "\n",
        "#training loop\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(epochs):\n",
        "  for i , (image , labels) in enumerate(train_loader):\n",
        "\n",
        "    image = image.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    #FP\n",
        "    output = model(image)\n",
        "    loss = criterion(output , labels)\n",
        "\n",
        "    #BP\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    if (i+1) % 10000 == 0:\n",
        "      print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "#testing and evaluation\n",
        "with torch.no_grad():\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "  n_class_correct = [0 for i in range(10)]\n",
        "  n_class_samples = [0 for i in range(10)]\n",
        "  for images , labels in test_loader:\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(images)\n",
        "    _, predict = torch.max(output , 1)\n",
        "    n_samples += labels.shape[0]\n",
        "    n_correct += (predict == labels).sum().item()\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      label = labels[i]\n",
        "      pred = predict[i]\n",
        "      n_class_correct[label] = n_class_correct[label]+1 if (label == pred) else n_class_correct[label]\n",
        "      n_class_samples[label] = n_class_samples[label]+1\n",
        "\n",
        "  acc = 100.0 * n_correct / n_samples\n",
        "  print(f'Accuracy = {acc}')\n",
        "\n",
        "  for i in range(10):\n",
        "    acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "    print(f'Accuracy of {classes[i]} : {acc}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRVShDzdeiDe"
      },
      "source": [
        "# ***Transfer Learning***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30f4a881"
      },
      "outputs": [],
      "source": [
        "# Transfer Learning: Reusing a pre-trained model on a new task.\n",
        "# Instead of training a deep learning model from scratch on a new dataset,\n",
        "# which requires a large amount of data and computational resources,\n",
        "# transfer learning leverages a model that has already been trained on a\n",
        "# similar, large dataset (e.g., ImageNet).\n",
        "# The idea is that the pre-trained model has already learned to extract\n",
        "# useful features (like edges, textures, shapes) from the data.\n",
        "# These learned features can then be transferred and adapted to the new task.\n",
        "# This is particularly useful when the new dataset is small, as it prevents\n",
        "# overfitting and speeds up training.\n",
        "# Common approaches include:\n",
        "# 1. Feature Extraction: Using the pre-trained model as a fixed feature extractor\n",
        "#    and training a new classifier on top of the extracted features.\n",
        "# 2. Fine-tuning: Unfreezing some or all layers of the pre-trained model\n",
        "#    and training them along with the new classifier on the new dataset,\n",
        "#    usually with a smaller learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDHDi5-ie9YV",
        "outputId": "91f36808-3e17-4279-d1f6-3a21c38e9cc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/utkarshsaxenadn/car-vs-bike-classification-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 103M/103M [00:00<00:00, 150MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/utkarshsaxenadn/car-vs-bike-classification-dataset/versions/1\n"
          ]
        }
      ],
      "source": [
        "#We will use PreTrainined ResNet image Classifier which\n",
        "#is trained on millions od data\n",
        "\n",
        "#Image Folder\n",
        "#Schedulers\n",
        "#transfer Learning\n",
        "\n",
        "\n",
        "#Download the Car and Bike dataset\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"utkarshsaxenadn/car-vs-bike-classification-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "import os\n",
        "path = (os.path.join(path , \"Car-Bike-Dataset\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVVHc7zzhAv6",
        "outputId": "35ace6a5-0488-4d46-926b-9fddf5ddd401"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import datasets, models , transforms\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mPAIROqiFAC",
        "outputId": "67723aa7-a498-4649-85c5-3c5f0b43ad8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " training Size : 3200 , Validation Size : 800\n",
            "Classes in Dataset : ['Bike', 'Car'] \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "mean = np.array([0.485 , 0.456 , 0.406])\n",
        "std = np.array([0.229 , 0.224 , 0.225])\n",
        "transform = transforms.Compose([transforms.ToTensor() , transforms.Normalize(mean , std) , transforms.RandomHorizontalFlip() , transforms.RandomVerticalFlip() , transforms.RandomRotation(90) , transforms.RandomResizedCrop(224)])\n",
        "\n",
        "dataset = datasets.ImageFolder(path, transform=transform )\n",
        "train , val = torch.utils.data.random_split(dataset , [int(0.8*len(dataset)) , len(dataset)-int(0.8*len(dataset))])\n",
        "\n",
        "print(f\" training Size : {len(train)} , Validation Size : {len(val)}\")\n",
        "print(f\"Classes in Dataset : {dataset.classes} \")\n",
        "train_dataloader = torch.utils.data.DataLoader(train , batch_size = 20 , shuffle = True , num_workers = 4)\n",
        "val_dataloader = torch.utils.data.DataLoader(val, batch_size = 20, shuffle = True , num_workers = 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SbZnJdz4-MxX"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train' , 'val']:\n",
        "            model.train() if phase == 'train' else model.eval()\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            total_samples = 0 # Keep track of total samples processed in the phase\n",
        "\n",
        "            dataloader = train_dataloader if phase == 'train' else val_dataloader\n",
        "            dataset_size = len(train) if phase == 'train' else len(val) # Use correct dataset size for averaging\n",
        "\n",
        "            for inputs , labels in dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs , 1)\n",
        "                    loss = criterion(outputs , labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        optimizer.zero_grad()\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                total_samples += inputs.size(0) # Accumulate total samples\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_size # Calculate loss over the entire phase\n",
        "            epoch_acc = running_corrects.double() / dataset_size # Calculate accuracy over the entire phase\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4PJDd0c4D6Z",
        "outputId": "a0065b4b-4a96-4475-d519-9dc3787f919b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|| 44.7M/44.7M [00:00<00:00, 188MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/4\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.3783 Acc: 0.8306\n",
            "val Loss: 0.2508 Acc: 0.9025\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.2775 Acc: 0.8872\n",
            "val Loss: 0.2297 Acc: 0.9075\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.2939 Acc: 0.8697\n",
            "val Loss: 0.1923 Acc: 0.9213\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.2527 Acc: 0.8909\n",
            "val Loss: 0.2247 Acc: 0.8950\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.2818 Acc: 0.8712\n",
            "val Loss: 0.2176 Acc: 0.9062\n",
            "\n",
            "Training complete in 6m 18s\n",
            "Best val Acc: 0.921250\n"
          ]
        }
      ],
      "source": [
        "#Fine Tunning\n",
        "model = models.resnet18(pretrained = True)\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False  #freezing all layers\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features , 2)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters() , lr = 0.001 , momentum = 0.9)\n",
        "\n",
        "\n",
        "#scheduler  -- update learning rate\n",
        "step_lr_scheduler = lr_scheduler.StepLR(optimizer , step_size = 7 , gamma = 0.1)\n",
        "\n",
        "model = train_model(model , criterion , optimizer , step_lr_scheduler , num_epochs = 5).to(device)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
